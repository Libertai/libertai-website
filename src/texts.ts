export const models: Array<{ title: string; description: string }> = [
	{
		title: "Nous Hermes 2 Pro (Llama 3 8B)",
		description:
			"Hermes 2 Pro is an upgraded, retrained version of Nous Hermes 2, consisting of an updated and cleaned version of the OpenHermes 2.5 Dataset, as well as a newly introduced Function Calling and JSON Mode dataset developed in-house.",
	},
	{
		title: "NeuralBeagle (7B)",
		description:
			"NeuralBeagle14-7B is a DPO fine-tune of mlabonne/Beagle14-7B using the argilla/distilabel-intel-orca-dpo-pairs preference dataset.",
	},
	{
		title: "Mixtral (8x7B MOE)",
		description: "Mixtral 8x7B, a high-quality sparse mixture of experts model (SMoE) with open weights.",
	},
	{
		title: "Nous Hermes 2 (34B)",
		description:
			"Nous Hermes 2 Yi 34B was trained on 1,000,000 entries of primarily GPT-4 generated data, as well as other high quality data from open datasets across the AI landscape.",
	},
	{
		title: "Llama 3 Instruct (70B)",
		description:
			"Llama 3 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks.",
	},
	{
		title: "DeepSeek Coder (6.7B)",
		description:
			"Deepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese.",
	},
];
